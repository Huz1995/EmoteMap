<p align="center">
<img src="supporting_images/EC.png" width="800px">
</p>

## Table of contents
1. [Evaluation](#eval)
* [a) Methodology](#method)
* [b) Timeline of Evaluation Methods](#timeline)
* [c) More Limitations: Sample Size](#success)

2. [Conclusion](#conc)

<a name="eval"></a>
# EVALUATION

<a name="method"></a>
# a) Methodology


In order to evaluate how our product would be received by users we used five techniques throughout the process; paper prototyping, wireframing, an idea matrix, questionnaires, semi-structured interviews.

*come back to this at the end
- intro/abstract
- why did you use certain methods of evaluation for your particular design - qual and quant
- summarise the rest of the section
- how did the different methods link with each other?*

Below is a short summary of our 5 main techniques of evaluating our design through users. Following this, we talk about them in more detail in our timeline of Evaluation where we discuss the kind of feedback we felt was necessary at different stages of the development process.

*perhpaps expand the following by:
- going into more detail on the explanation front
- ensuring each headings' links with the other headings are explained
- give an example for the things you can make examples from
- -	talk in more detail about each thing e.g â€“ Paper Prottyping wizard of oz
- ensure you are talking about 'what is this method?'*

### Evaluation Technique 1: Paper prototyping


<p>
<img src="supporting_images/Paper_prototype_start.jpeg" width="250" align="left">
This process involved drawing our first interpretations of what we envisaged our website to look like on paper and then showing and observing potential users' interaction with it. The process of paper prototyping involved three roles..


**1. User:**
*The user was the person that would be trying the system, saying out loud what they were thinking as they tried to navigate the paper prototype*

**2. Observer:**
*This person watched from a distance away and did not interact or say anything to other people taking part in the process. However, they maintained an important role here, which was to take notes on any critical incidents. This way, we could gauge any parts of our early ideation that strongly effected the usability of our design.*

**3. Facilitator:**
*This person acted as the leader of process, guiding the user through the prototype and asking key questions along the way. It was their job to encourage the user to think aloud by asking questions about their thought pattern such as "Do you know what is going on at this point?" or "What would you do next now that you are on the Sign Up page?". Or perhaps if the user was stuck (which happened a lot at the stage of paper prototyping!), "Do you find anything here useful, in terms of figuring out what you would like to do next?"*

A limitation here was that Our teams' paper prototyping was limited by the need to social distance. Members of our team were only able to work with two family members or housemates. Despite emphasising to any one participating to act as impartially as possible, it  can be argued that the fact that people lived with us helped us carry out this process was detrimental because we needed the user to be as truthful as possible, whilst people we live with were more likely to tell us what we wanted to hear about our designs.



To learn more about how our paper prototyping allowed us, at a very early stage, obtain our first UX feedback and its essential role in our early ideation see the [UX section](uxDesign.md) , where we give specific examples of user preferences after seeing our paper prototype which then went on to influence our design process.

</p>


### Evaluation Technique 2: Wireframing
<p>
<img src="supporting_images/wireframe_two.png" width="400" align="left">

Wireframing enabled us to create an interactive version of our undeveloped website that users could access on a desktop easily through a link [like this one](https://zaki744910.invisionapp.com/console/share/4Y2FIC1NV7/584973219). For example, we could draw parts of the website that we had not yet built onto screenshots of the website as it looked at the time.

Please see our [UX section](uxDesign.md) where we discuss how users were able to engage with our website and leave comments on what they thought about their experience with it.
</p>

<p align="center">
<img src="supporting_images/feedback.png" width="600">
</p>

As demonstrated by the screenshot of some feedback a user left, a limitation that we experienced when sending out the link to users can be seen. In this case, the 'x' on the comment that they are referring to was part of the screenshot of the webpage that we had uploaded as part of wireframe. Thus, this user did not understand the concept of a wireframe and thought that it was more interactive and accurate than it was. Due to the fact that some users thought  the wireframe should work in the same way as the website would, we recognise that perhaps we should have either been clearer when sending out the message with the link in or carried out more guided walkthroughs of the wireframe in order to avoid this issue.    

Below is a screenshot of a text we sent to people who agreed to look at our wireframe and then fill out a questionnaire.

<p align="center">
<img src="supporting_images/imessage.png" width="400">
</p>





### Evaluation Technique 3: Idea Matrix
<p align="center">
<img src="supporting_images/idea_matrix.png" width="600">
</p>

Another method we used in evaluating our designs was an idea matrix. This was an excel spreadsheet that came to be extremely useful in prioritising user suggestions and thus which changes we were going to make to the website throughout the development process.

### Evaluation Technique 4: Questionnaires


<p>
<img src="supporting_images/questionnaire.png" width="550" align="left">
Our grouped used questionnaires as our main form of quantitative feedback as we were able to send a link, such as [this one](https://www.smartsurvey.co.uk/s/S58J8X/) to as many users as we could who had revently used one of our wireframes or been shown our most up to date website on a local host. This way, we could gain insights at scale into how users interpreted our website through them answering questions such as "Would you recommend this product to a friend?", "Do you think this website is solving a serious problem?" or "How would you improve this website?".

In total we produced three questionnaires. The first two were sent to users alongside the wireframes and the last one was filled out by users after they had used the website on a local host. The last two questionnaires asked the same questions as the first, but we improved it slightly. For example, we added a 'if not, why not' option to answers that currently only included 'yes' and 'no' so that we did not force the user to have to take longer with it but gave them the option to leave more detailed feedback if they wished.

*Here are links to all three of our questionnaires if you are interested:*  
 [1st questionnaire (1st wireframe)](https://www.smartsurvey.co.uk/s/S58J8X/)  
 [2nd questionnaire (2nd wireframe)](https://www.smartsurvey.co.uk/s/S58J8X/)  
 [3rd questionnaire (Our most up to date website shown on a local host)](https://www.smartsurvey.co.uk/s/S58J8X/)
</p>

<p align="left">
<img src="supporting_images/right.png" width="600">
</p>
<p align="right">
<img src="supporting_images/left.png" width="600">
</p>




The questionnaires themselves did in fact hold a qualitative functionality to them. This was mainly in two of the questions, seen above, where we prompted the user for a response.


### Evaluation Technique 5: Semi-structured interviews
<p>
<img src="supporting_images/postPosition.png" width="400" align="right">
We carried out semi-structured interviews as a method of gathering qualitative feedback data. Our interviews were on average 10 minutes long and were conducted after someone had interacted with our paper prototype, a wireframe, or the most up to date version on a local host. The questions we asked in each interview varied depending on the stage of the development process that we found ourselves at. We were able to ask very specific questions about our UX this way. For example, at a relatively early stage of the development process we were trying to decide whether to have the post landing page appear from the left hand side. As seen by the images to the right , we were able to show the user two potential layouts that we sketched on the website [Invision](https://www.invisionapp.com). Being able hear their reasons behind their decisions about our user interface was extremely useful.

On top of this, we would always ensure that we asked a general and open ended question at the end of the interview about how the user thought we could improve the user experience as it was, or if they had any ideas. That being said, something extremely insightful that we noticed when conducting interviews was that the user would constantly be making comments about how they would want the website to be improved as they were interacting with the wireframe or prototype. Due to the fact
that we recorded the audio to our interviews, we were able to make use out of any suggestions.
</p>




















<a name="timeline"></a>
# b)Timeline of Evaluation Methods
<p align="center">
<img src="supporting_images/timeline.png" width="1000">
</p>

### Stage 1: Early Ideation & Paper Prototyping 21/2/21
21/2/21: Paper prototyping was a key part of getting our idea off the ground and taking an abstract concept in our heads to a more consolidated one. Although our idea of how the website would look was completely premature, this was our first attempt at getting a feel for how users would interact with the website. Users were able to give us a sense of direction in this way before we started coding anything

24/2/21: Paper prototyping naturally led us to start conducting semi-structured interviews. The process of paper prototyping, as explained above, somewhat resembled the process of an interview. As a group, we ensured that we conducted such interviews with people after they had undergone the paper prototyping from this date onwards. This method of qualitative evaluation continued throughout the rest of our development process, being conducted with users after they had seen the wireframe or been shown the most up to date version of the website on a local host.

### Stage 2: Move to Wireframing 17/3/21
<p align="center">
<img src="supporting_images/wireframe_one.png" width="500">
</p>


17/3/21: So far, our evaluation was based on feedback from users who had only seen our ideas written on paper. At the point that we had built the first map interface part we were able to take our prototyping one step further by showing them a wireframe where we were able to make the undeveloped website look as it would do in the future. We decided to use a wireframe because it allowed us to evaluate how users thought of our design before it was actually coded.

<p align="center">
<img src="supporting_images/wireframe_2.png" width="500">
</p>


5/4/21: There reached a point where our first wireframe became out of date in relation to the level of development of the website. At the same time, we felt that the website was too undeveloped to be used by users. Therefore, we built a second wireframe so we could continue to evaluate our website against user preferences which enabled us to gain feedback from users at different stages during the development process as we built our website ahead of time.


### Stage 3: Quantitative Feedback Involvement 24/3/21
24/3/21: Although we felt that interviews were a good source of creating qualitative feedback, we made a decision to try and increase our sample size when it came to evaluating our product and attempt to create some quantitative feedback. In order to achieve a quantitative metric for how a user would feel about our product, we decided it was necessary to ask some 'how much' and 'how many' questions at scale to our users. Very shortly after we started showing our first wireframe to users, we created a questionnaire that we could send alongside the link to the wireframe. We tried to push the quantitative aspect of the questionnaires by using 5 point likert scale, as seen in the above figure. For example, in improving our first questionnaire to our second one,  ensured that we reflected the different points on the scale itself, instead of having just numbers. For example as possible choices to the question 'How playful did you find this website?', instead of just having options 1-5, we would have:        
1 - not playful at all  
2 - not very playful  
3 - somewhat playful  
4 - very playful  
5 - extremely playful  



### Stage 4: Idea Matrix  30/3/21
30/3/21: Shortly after we started collecting quantitative data, we decided that it would be effective to create a method by which we could better draw insights from it. We made this decision because at this point we had received many suggestions from users in terms of how we could improve the user interface of our design. In this way we were able to populate the idea matrix with the recorded feedback data from both the questionnaires and semi-structured interviews.

Although the idea matrix complemented both the semi-structured interview feedback and the questionnaire data, we found that it was particularly useful for the last question on the questionnaire: "How would you go about improving the website?(e.g - is there something missing to you?)"


### Stage 5: Presenting Webpage to Users on Local Host 5/4/21

We continued using wireframes as a method of evaluation for our designs until our website was functional enough to show users via a local host. We felt that our website was developed enough by 27/3/21 (see sprint 3 in the sprints section for more information on exactly how developed the website was at this point). However there was a period where all team members were busy which meant that we conducted interviews and collected questionnaire responses for the users that were shown the webpage on the local host slightly later than expected.    

<p align="left">
<img src="supporting_images/navigation_left.png" width="500">
</p>
<p align="right">
<img src="supporting_images/navigation_right.png" width="500">
</p>

For our group, we found this stage particularly significant as our design involved the navigation of a map, and the creation of a post which contained a personal message. Due to this nature of our design, there is quite a personalised element to how a user interacts with it. Thus, for our user feedback to be more accurate and meaningful, having users experience the actual website was significant to evaluating and improving our designs in the development process. In agreement with this, we found that users found it much easier to navigate the website in this way, as opposed to using the wireframe. See the screenshot above which shows the feedback from the last two questionnaires which proves this.


<p align="left">
<img src="supporting_images/numberofparticipants.png" width="600">
Another possible limitation to our evaluation was our inability to show enough users our most up to date version of the product when we moved to doing so on a local host as opposed to a wireframe. In order to do this, we would need to either show someone in person or organise a Microsoft Teams or Zoom meeting which proved time consuming and less successful when it came to quantitative data collection. It was much easier for a user to use the link to a wireframe and then fill out the questionnaire. As seen in the image to the left, this is demonstrated by the fact that we had an average of 8.5 responses for the first two wireframes, as opposed to 5 for the final one. This being said, all the users who carried out a questionnaire had been shown the website personally, and also been interviewed. As a result, this user feedback was of greater richness and therefore the qualitative element of it was more valuable.
</p>













<a name="success"></a>
# More Limitations: Sample Size
*screenshot of sample size calc*
*screenshot of figure for actual sample data*
We used Survey Monkey's sample size calculator to calculate our sample size. This forced us to think about the population that may be using our product and therefore question how big our market was. Considering our end users in this way made us contemplate how much feedback data was enough.  As seen in the figure above, our sample size was *X* whilst  our actual sample size was *x*. Although we did not reach the ideal sample size, we realised that we could only ultimately analuse the data that we could gather. This shortfall of responders therefore makes it difficult to be certain of our analysis at this point.
Although our sample size could have been higher, we followed the notion that when carrying out our evaluation we would balance richness with practicality (*reference lecture*). When creating our feedback data we were constantly considering the analysis phase and how we would go on to analyse that data. In order to thoroughly and accurately analyse our data in a just way, we dedicated time. As a result of this, we at some points opted to collect a smaller amount of qualitative data which we could analyse robustly. For example, carrying out qualitative user feedback such as semi-structured interviews was time-consuming as we needed to spend time re-listening to the audio and extracting the relevant responses to questions before entering this feedback data into the idea matrix. Ultimately, although our feedback data lacked in terms of sample size because of this, we were able to make the analysis of the data much richer.    






<a name="conc"></a>
# CONCLUSION
scope and future work must go here...
Perhaps hometown / lived location could be something you grab from the user too
(1 liked)
some more things we can add to the documentation
this is really important guys
this feedback
"I mean a user history mode that is mapped could be quite interesting, however, there is one thing I want you to think about (not necessarily to develop anything, but just to think about for your future work section of the report). Simply, how will this data be used and who will use it?

If I clicked on your Edgware Earthquake post, how could I support you? This is where you make the jump from raising awareness, to going further

### Reflect on the working practices of your group, how well they did or did not work, e.g, management of issues, communication, Agile (etc).

Taken as a whole, we worked quite effectively as team, with the clear team roles and constant communication over discord helping greatly in facilitating collaboration and continuous integration, as we were very quickly able to discuss issues and solutions, and delegate who should should be tasked with implementing those solutions.

We did have issues in the first couple of sprints around effectively using github and angular, as most of the team were still getting to grips with both technologies, but once we implemented a rigid process of fetching and pulling from our repository before and after every local change, these issues subsided.

As discussed in our [Project Management & development process](sprints.md) document, the use of discord channels greatly aided discussion as we were able to keep focused the discussion on disperate elements of the project, instead of having one chat where everything was lumped into one place, meaning that we were able to keep track of and refer to implementation issues in specific areas.

### Reflective discussion of the success of the project. How well did our project fulfil the brief? Were all of our objectives met?

On analysis of our qualitative data, we were able to conclude that throughout the process of feedback and the subsequent development of software, users started to see the purpose of the website as being more in line with what we initially set out for it to be. For us, this somewhat represented the success of the process behind how we evaluated user's perceptions of our product.
*1. to see/ineract with other usersâ€™ posts (e.g â€“ to gain insight...)
-	2. to view trends in experienced emotions globally
-	screenshot of ability to see othersâ€™ posts on the website
-	screenshot of comment for this point*  

Broadly, we would argue that our project has been a success, both in meeting the brief and fulfilling our own objectives, although more so in some areas than others. The project brief was to develop a web app around the concept of 'serious play'; in our case we took that to mean bringing attention to a real world problem in a new and engaging - a playful - way, along with the prehaps enabling ways to address the problem.

As a team, we view the display of emotion via a map a novel and interactive way to learn about mental health issues, and think that the application in its current state, once it has more users, has great potential to facilitate the sharing of emotions on a larger scale. Feedback we collected broadly agreed with us, with 100% of responses to our minimal viable product (mvp) website rating our website 3/5 or higher on a scale with 1 representing "mind-numbingly boring" and 5 being "extremely interactive" when asked how playful our website is. Similarly, when asked if our website was addressing a serious problem on a scale of 5, 100% ranked the website 3/5 or higher.

Clearly however there is room for improvements to be made; though the current map display makes it clear where people are feeling 'happy' or 'sad', it is less successful at presenting the reason FOR these broader trends. Though users can hover over posts to see the reason for a particular rating, our website does not currently coalesce these reasons. Adding this broader trend element would make our website more playful, and greatly improve the extent to which we met our 3rd objective -  "creating a tool that raises awareness and information through a visual heat-map based on an accumulation of users' mental health in different geolocations" - however we would still argue that our user feedback suggests we did achieve some success in meeting this goal. We also were clearly successful in this goal in terms of technical achievement; our website does have a visual heat-map effect displaying trends in user emotions.


Our first objective was to "create a writing tool that allows self-reflection in a similar way to blogging". We would argue that our project was very successful in meeting this goal; even disregarding the map element of our website, the app allows users a clear way to make a 'blog post', and has a dated timeline/history of all previous posts, essentially providing the function of a virtual diary. The mapping and mood elements actually provide additional functionality, with users able to see their emotion and location for each 'blog' post they have made - we argue that this could make our website better than a traditional diary/blogging service for facilitating self-reflection. However, our user feedback suggests that most people would not user our website in this way; only 20% of people were most likely to use the website for blogging purposes, although this does not mean that users wouldn't use it for this purpose, just that they found the other features more engaging.

Finally, our second objective was to "create a peer-led support network where users can interact with each other as a community". For this objective we did achieve some level of success, with some caveats. Our user feedback suggested that people were keen to use the website in a social manner; 60% of users said they were most likely to use the website "To see/interact with other users' posts (e.g - to gain insight about reactions to similar life experiences)". This also further supports us achieveing success in creating a playful application; users wanted to use the website to engage with with peoples emotions and experiences! However we would argue that beyond the comradery provided by seeing other users experiencing similar emotions, in its current state our website does not provide much, if any, interaction between users beyond this. In the future work section below we discuss some ideas for features that would help us better achiveve this objective.



### This is a chance to reflect on how coronavirus has affected your project (remote working practices etc)

Throughout the entirety of this project the UK has been in some degree of national lock down. Due to this, it has not been possible for the team to meet up in person and therefore all interaction within the team has taken place online. This has presented challenges. Without face to face interaction it is more difficult to form personal bonds which can help drive team morale. On top of this being limited to online interaction means jobs can tend to take longer to complete and ideas are more difficult to bounce of each other. To counteract this, we decided early on how we would collaborate and set up the infrastructure we would would require to do so.

### Discussion of Social and Ethical implications of your work.

### Discussion of future work (in terms of design, development and evaluation)

One extension idea we have involves further inter-user interaction and communication in a local area. Here is the idea, demonstrated with an example: 'Bristol is notorious for its limited parking. On a particular residential road, there is a bus stop that has been out-of-service for years. Despite paying for a parking permit, residents have been told that they are not allowed to park on the bus stop even if that means parking on another road. Residents are upset as they often find themselves struggling to park after a busy day at work. Naturally, they individually take to EmoteMap to express their feelings. To their surprise, they discover that others feel the same way. This is because EmoteMap has identified multiple people within an area expressing similar feelings over the same issue. EmoteMap then asks the individuals if they would like to initialise a discussion group, in hope of resolving the local issue.' This extension would involve the ability to identify multiple users within a local area reporting the same issue. Furthermore, some form of chat service would need to be implemented. !!!!Perhaps we could briefly mention what tools could be used to achieve this?!!!!!!!

At a late stage in our development, one user made an important point on privacy: they were concerned about sharing their location when making an Emote Post. This is a valid concern and we questioned how we may be able to deal with this issue in future development of the project. Submitting Emote Posts without some form of geolocation voids the project's idea entirely. We therefore propose an alternative direction: to give users the ability to pin their posts to an area (such as a district), instead of a precise location. We believe this would be a good balance between privacy, and having the ability to see trends in emotion across different areas of the globe. It would be interesting to survey whether such a feature would encourage those previously concerned over privacy to then use the site.


focus on mens mental health


mention how we want to do more testing than stated in the test file (see td)
