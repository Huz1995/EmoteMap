## Table of contents
1. [Evaluation](#eval)
* [a) Methodology](#method)
* [b) Timeline of Evaluation Methods](#timeline)
* [c) How Successful was our Evaluation of User Feedback?](#success)

2. [Conclusion](#conc)

<a name="eval"></a>
# EVALUATION

<a name="method"></a>
# a) Methodology


In order to evaluate how our product would be received by users we used five techniques throughout the process; paper prototyping, wireframing, an idea matrix, questionnaires, semi-structured interviews.

*come back to this at the end
- intro/abstract
- why did you use certain methods of evaluation for your particular design - qual and quant
- summarise the rest of the section
- how did the different methods link with each other?*

Below is a short summary of our 5 main techniques of evaluating our design through users. Following this, we talk about them in more detail in our timeline of Evaluation where we discuss the kind of feedback we felt was necessary at different stages of the development process.

*perhpaps expand the following by:
- going into more detail on the explanation front
- ensuring each headings' links with the other headings are explained
- give an example for the things you can make examples from
- -	talk in more detail about each thing e.g â€“ Paper Prottyping wizard of oz
- ensure you are talking about 'what is this method?'*

### Evaluation Technique 1: Paper prototyping


<p>
<img src="supporting_images/Paper_prototype_start.jpeg" width="250" align="left">
This process involved drawing our first interpretations of what we envisaged our website to look like on paper and then showing and observing potential users' interaction with it. The process of paper prototyping involved three roles..  


**2. User**
*The user was the person that would be trying the system, saying out loud what they were thinking as they tried to navigate the paper prototype*    

**3. Observer**
*This person watched from a distance away and did not interact or say anything to other people taking part in the process. However, they maintained an important role here, which was to take notes on any critical incidents. This way, we could gauge any parts of our early ideation that strongly effected the usability of our design.*    

A limitation here was that Our teams' paper prototyping was limited by the need to social distance. Members of our team were only able to work with two family members or housemates. Despite emphasising to any one participating to act as impartially as possible, it  can be argued that the fact that people lived with us helped us carry out this process was detrimental because we needed the user to be as truthful as possible, whilst people we live with were more likely to tell us what we wanted to hear about our designs.



To learn more about how our paper prototyping allowed us, at a very early stage, obtain our first UX feedback and its essential role in our early ideation see the [UX section](uxDesign.md) , where we give specific examples of user preferences after seeing our paper prototype which then went on to influence our design process.

</p>



### Evaluation Technique 2: Wireframing

Wireframing enabled us to create an interactive version of our undeveloped website that users could access on a desktop through a link [like this one for our second wireframe!](https://zaki744910.invisionapp.com/console/share/4Y2FIC1NV7/584973219). For example, we could draw parts of the website that we had not yet built onto screenshots of the website as it looked at the time.  
Please see our [UX section](uxDesign.md) where we discuss how users were able to engage with our website and leave comments on what they thought about their experience with it.

### Evaluation Technique 3: Idea Matrix
*picture of idea matrix up to date*

Another method we used in evaluating our designs was an idea matrix. This was an excel spreadsheet that came to be extremely useful in prioritising what changes we were going to make to the website throughout the development process.

### Evaluation Technique 4: Questionnaires

Our group used questionnaires both as a qualitative and quantitative method of evaluating our designs. It was our main form of quantitative feedback as we were able to send a link *insert link* to as many users as we could who had just used one of our wireframes or been shown our most up to date website on a local host. This way, we could gain insights at scale into how users interpreted our website through them answering questions such as "Would you recommend this product to a friend?", "Do you think this website is solving a serious problem?" or "How would you improve this website?".   


### Evaluation Technique 5: Semi-structured interviews  
These interviews were carried out throughout the development process all the way from paper-prototyping at the beginning, to presenting the website on a local host in its most complete form. We saw interviews as the main form of qualitative feedback.

<a name="timeline"></a>
# b)Timeline of Evaluation Methods  
<p align="center">
<img src="supporting_images/Paper_prototype_start.jpeg" width="400">
</p>

### Stage 1: FEEDBACK ON EARLY IDEATION 21/2/21
21/2/21: Paper prototyping was a key part of getting our idea off the ground and taking an abstract concept in our heads to a more consolidated one. Although our idea of how the website would look was completely premature, this was our first attempt at getting a feel for how users would interact with the website. Users were able to give us a sense of direction in this way before we started coding anything

24/2/21: Paper prototyping naturally led us to start conducting semi-structured interviews. The process of paper prototyping, as explained above(link to above), somewhat resembled the process of an interview. As a group, we ensured that we conducted such interviews with people after they had undergone the paper prototyping from this date onwards. This method of qualitative evaluation continued throughout the rest of our development process, being conducted with users after they had seen the wireframe or been shown the most up to date version of the website on a local host.

### Stage 2: MOVE TO WIREFRAME 17/3/21
[First wireframe](/Users/zakigill/Desktop/1st Wireframe.png)  
17/3/21: So far, our evaluation was based on feedback from users who had only seen our ideas written on paper. At the point that we had built the first map interface part we were able to take our prototyping one step further by showing them a wireframe where we were able to make the undeveloped website look as it would do in the future. We decided to use a wireframe because it allowed us to evaluate how users thought of our design before it was actually coded.

[Second Wireframe](/Users/zakigill/Desktop/2nd wireframe.png)   
5/4/21: There reached a point where our first wireframe became out of date in relation to the level of development of the website. At the same time, we felt that the website was too undeveloped to be used by users. Therefore, we built a second wireframe so we could continue to evaluate our website against user preferences which enabled us to gain feedback from users at different stages during the development process as we built our website ahead of time.  

*give a questionnaire stats between the two wireframes*

### Stage 3: INVOLVEMENT OF QUANTITATIVE FEEDBACK 24/3/21
*screenshot of 5 point likert scale*  

24/3/21: Although we felt that interviews were a good source of creating qualitative feedback, we made a decision to try and increase our sample size when it came to evaluating our product and attempt to create some quantitative feedback. In order to achieve a quantitative metric for how a user would feel about our product, we decided it was necessary to ask some 'how much' and 'how many' questions at scale to our users. Very shortly after we started showing our first wireframe to users, we created a questionnaire that we could send alongside the link to the wireframe. Click on this link to see it! *link the questionnaire*. We tried to push the quantitative aspect of the questionnaires by using 5 point likert scale, as seen in the above figure.  

*screenshot of  question 7*
The questionnaires themselves did in fact hold a qualitative functionality to them. This was mainly in two of the questions, seen above, where we prompted the user for a response.  


In total we produced three  questionairres. The first two were sent to users alongside the wireframes and the last one was filled out by users after they had used the website on a local host. The last two questionnaires asked the same questions as the first, but we improved it slightly. *Give example of how you improved this*  



### Stage 4: INVOLVEMENT OF IDEA MATRIX 30/3/21
30/3/21: Shortly after we started collecting quantitative data, we decided that it would be effective to create a method by which we could better draw insights from it. We made this decision because at this point we had received many suggestions from users in terms of how we could improve the user interface of our design. In this way we were able to populate the idea matrix with feedback data from both the questionnaires and semi-structured interviews.  

Although the idea matrix complemented both the semi-structured interview feedback and the questionnaire data, we found that it was particularly useful for the last question on the questionnaire (see below)  

*screenshot of last question on questionnaire*


### Stage 5: MOVE FROM WIREFRAMING TO PRSENTING TO USERS VIA LOCALHOST 5/4/21

We continued using wireframes as a method of evaluation for our designs until our website was functional enough to show users via a local host. We felt that our website was developed enough by 27/3/21 (see sprint 3 in the sprints section for more information on exactly how developed the website was at this point). However there was a period where all team members were busy which meant that we conducted semi-structured interviews and collected questionnaire responses slightly later than expected.  

The way in which we obtained user feedback here was similar to that of the wireframes. However, we could only collect data from the questionnaire every time we showed a user the local host in person. This made it more difficult to get a substantial number of users to give their opinion than it was for the wireframes. In an attempt to combat this, we organised some zoom meetings where we could screenshare. *upload a screenshot of this*

*stat difference between 2nd wireframe and what users thought when using local host*

<a name="success"></a>
# c)How Successful was our Evaluation of User Feedback?

On analysis of our qualitative data, we were able to conclude that throughout the process of feedback and the subsequent development of software, users started to see the purpose of the website as being more in line with what we initially set out for it to be. For us, this somewhat represented the success of the process behind how we evaluated user's perceptions of our product.    

However, there were some limiting factors and debatable areas of improvement to this process that we are aware of.  

*1. to see/ineract with other usersâ€™ posts (e.g â€“ to gain insight...)
-	2. to view trends in experienced emotions globally
-	screenshot of ability to see othersâ€™ posts on the website
-	screenshot of comment for this point*


### Limitations
We used Survey Monkey's sample size calculator to calculate our sample size. This forced us to think about the population that may be using our product and therefore question how big our market was. Considering our end users in this way made us contemplate how much feedback data was enough.  As seen in the figure above, our sample size was *X* whilst  our actual sample size was *x*. Although we did not reach the ideal sample size, we realised that we could only ultimately analuse the data that we could gather. This shortfall of responders therefore makes it difficult to be certain of our analysis at this point.  
Although our sample size could have been higher, we followed the notion that when carrying out our evaluation we would balance richness with practicality (*reference lecture*). When creating our feedback data we were constantly considering the analysis phase and how we would go on to analyse that data. In order to thoroughly and accurately analyse our data in a just way, we dedicated time. As a result of this, we at some points opted to collect a smaller amount of qualitative data which we could analyse robustly. For example, carrying out qualitative user feedback such as semi-structured interviews was time-consuming as we needed to spend time re-listening to the audio and extracting the relevant responses to questions before entering this feedback data into the idea matrix. Ultimately, although our feedback data lacked in terms of sample size because of this, we were able to make the analysis of the data much richer.  
Another possible limitation to our evaluation was our inability to show enough users our most up to date version of the product when we moved to doing so on a local host as opposed to a wireframe. In order to do this, we would need to either show someone in person or organise a Microsoft Teams or Zoom meeting which proved time consuming and less successful when it came to quantitative data collection. It was much easier for a user to use the link to a wireframe and then fill out the questionnaire. This is demonstrated by the fact that we had *X questionnaire completions for the final questionnaire as opposed to the first two*.

*screenshot of sample size calc*
*screenshot of figure for actual sample data*



<a name="conc"></a>
# CONCLUSION
scope and future work must go here...
Perhaps hometown / lived location could be something you grab from the user too
(1 liked)
some more things we can add to the documentation
this is really important guys
this feedback
"I mean a user history mode that is mapped could be quite interesting, however, there is one thing I want you to think about (not necessarily to develop anything, but just to think about for your future work section of the report). Simply, how will this data be used and who will use it?

If I clicked on your Edgware Earthquake post, how could I support you? This is where you make the jump from raising awareness, to going further

### Reflect on the working practices of your group, how well they did or did not work, e.g, management of issues, communication, Agile (etc).

Taken as a whole, we worked quite effectively as team, with the clear team roles and constant communication over discord helping greatly in facilitating collaboration and continuous integration, as we were very quickly able to discuss issues and solutions, and delegate who should should be tasked with implementing those solutions.

We did have issues in the first couple of sprints around effectively using github and angular, as most of the team were still getting to grips with both technologies, but once we implemented a rigid process of fetching and pulling from our repository before and after every local change, these issues subsided.

As discussed in our [Project Management & development process](sprints.md) document, the use of discord channels greatly aided discussion as we were able to keep focused the discussion on disperate elements of the project, instead of having one chat where everything was lumped into one place, meaning that we were able to keep track of and refer to implementation issues in specific areas.

### Reflective discussion of the success of the project. How well did our project fulfil the brief? Were all of our objectives met?

Broadly, we would argue that our project has been a success, both in meeting the brief and fulfilling our own objectives, although more so in some areas than others. The project brief was to develop a web app around the concept of 'serious play'; in our case we took that to mean bringing attention to a real world problem in a new and engaging - a playful - way, along with the prehaps enabling ways to address the problem.

As a team, we view the display of emotion via a map a novel and interactive way to learn about mental health issues, and think that the application in its current state, once it has more users, has great potential to facilitate the sharing of emotions on a larger scale. Feedback we collected broadly agreed with us, with 100% of responses to our minimal viable product (mvp) website rating our website 3/5 or higher on a scale with 1 representing "mind-numbingly boring" and 5 being "extremely interactive" when asked how playful our website is. Similarly, when asked if our website was addressing a serious problem on a scale of 5, 100% ranked the website 3/5 or higher.

Clearly however there is room for improvements to be made; though the current map display makes it clear where people are feeling 'happy' or 'sad', it is less successful at presenting the reason FOR these broader trends. Though users can hover over posts to see the reason for a particular rating, our website does not currently coalesce these reasons. Adding this broader trend element would make our website more playful, and greatly improve the extent to which we met our 3rd objective -  "creating a tool that raises awareness and information through a visual heat-map based on an accumulation of users' mental health in different geolocations" - however we would still argue that our user feedback suggests we did achieve some success in meeting this goal. We also were clearly successful in this goal in terms of technical achievement; our website does have a visual heat-map effect displaying trends in user emotions.


Our first objective was to "create a writing tool that allows self-reflection in a similar way to blogging". We would argue that our project was very successful in meeting this goal; even disregarding the map element of our website, the app allows users a clear way to make a 'blog post', and has a dated timeline/history of all previous posts, essentially providing the function of a virtual diary. The mapping and mood elements actually provide additional functionality, with users able to see their emotion and location for each 'blog' post they have made - we argue that this could make our website better than a traditional diary/blogging service for facilitating self-reflection. However, our user feedback suggests that most people would not user our website in this way; only 20% of people were most likely to use the website for blogging purposes, although this does not mean that users wouldn't use it for this purpose, just that they found the other features more engaging.

Finally, our second objective was to "create a peer-led support network where users can interact with each other as a community". For this objective we did achieve some level of success, with some caveats. Our user feedback suggested that people were keen to use the website in a social manner; 60% of users said they were most likely to use the website "To see/interact with other users' posts (e.g - to gain insight about reactions to similar life experiences)". This also further supports us achieveing success in creating a playful application; users wanted to use the website to engage with with peoples emotions and experiences! However we would argue that beyond the comradery provided by seeing other users experiencing similar emotions, in its current state our website does not provide much, if any, interaction between users beyond this. In the future work section below we discuss some ideas for features that would help us better achiveve this objective.



### This is a chance to reflect on how coronavirus has affected your project (remote working practices etc)

Throughout the entirety of this project the UK has been in some degree of national lock down. Due to this, it has not been possible for the team to meet up in person and therefore all interaction within the team has taken place online. This has presented challenges. Without face to face interaction it is more difficult to form personal bonds which can help drive team morale. On top of this being limited to online interaction means jobs can tend to take longer to complete and ideas are more difficult to bounce of each other. To counteract this, we decided early on how we would collaborate and set up the infrastructure we would would require to do so.

### Discussion of Social and Ethical implications of your work.

### Discussion of future work (in terms of design, development and evaluation)

One extension idea we have involves further inter-user interaction and communication in a local area. Here is the idea, demonstrated with an example: 'Bristol is notorious for its limited parking. On a particular residential road, there is a bus stop that has been out-of-service for years. Despite paying for a parking permit, residents have been told that they are not allowed to park on the bus stop even if that means parking on another road. Residents are upset as they often find themselves struggling to park after a busy day at work. Naturally, they individually take to EmoteMap to express their feelings. To their surprise, they discover that others feel the same way. This is because EmoteMap has identified multiple people within an area expressing similar feelings over the same issue. EmoteMap then asks the individuals if they would like to initialise a discussion group, in hope of resolving the local issue.' This extension would involve the ability to identify multiple users within a local area reporting the same issue. Furthermore, some form of chat service would need to be implemented. !!!!Perhaps we could briefly mention what tools could be used to achieve this?!!!!!!!

At a late stage in our development, one user made an important point on privacy: they were concerned about sharing their location when making an Emote Post. This is a valid concern and we questioned how we may be able to deal with this issue in future development of the project. Submitting Emote Posts without some form of geolocation voids the project's idea entirely. We therefore propose an alternative direction: to give users the ability to pin their posts to an area (such as a district), instead of a precise location. We believe this would be a good balance between privacy, and having the ability to see trends in emotion across different areas of the globe. It would be interesting to survey whether such a feature would encourage those previously concerned over privacy to then use the site.


focus on mens mental health
